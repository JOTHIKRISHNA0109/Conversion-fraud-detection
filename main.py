# -- coding: utf-8 --
"""Fraud Adversial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16mRPETyYQjT-rY5ZvBW3nqbGwyQ0BQoG
"""
#Importing dependencies
import numpy as np
import pandas as pd 
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

#Concatenating train and test dataset for uniform labeling

link="/content/Training Data.csv"
data=pd.read_csv(link)

data_test=pd.read_csv('/content/Test Data.csv')
Z=data.append(data_test)

#label encoding the columns utilized

def encode1(column_name):
  '''function to label encode the required parameters'''
  label_encoder = preprocessing.LabelEncoder()
  Z[column_name]= label_encoder.fit_transform(Z[column_name])
  return Z[column_name]

#Labeling 'Y' Parameter with 0's and 1's
label_encoder = preprocessing.LabelEncoder()
Z['conversion_fraud']= label_encoder.fit_transform(Z['conversion_fraud'])
Z['conversion_fraud']

#Labeling essential attributes for 'X' parameter
Z['clientid_cr']=encode1('clientid_cr')
Z['adslotdumid']=encode1('adslotdimid_cr')
Z['itemid_cr']=encode1('itemid_cr')
Z['goalid_cr']=encode1('goalid_cr')
Z['adLogType_cr']=encode1('adLogType_cr')
Z['pricingtype_cr']=encode1('pricingtype_cr')
Z['pubclientid_cr']=encode1('pubclientid_cr')
Z['siteId_cr']=encode1('siteId_cr')
Z['cityId_cr']=encode1('cityId_cr')
Z['browserId_cr']=encode1('browserId_cr')
Z['ispDimId_cr']=encode1('ispDimId_cr')
Z['osVerDimId_cr']=encode1('osVerDimId_cr')
Z['itemcolumbiaid_cr']=encode1('itemcolumbiaid_cr')
Z['algo_cr']=encode1('algo_cr')

#Z2 as output parameter and Z1 as input parameter
Z2=Z['conversion_fraud']
Z1=Z[['clientid_cr','adslotdimid_cr','itemid_cr','goalid_cr','adLogType_cr','pricingtype_cr','pubclientid_cr','siteId_cr','cityId_cr','browserId_cr','ispDimId_cr','osVerDimId_cr','algo_cr','itemcolumbiaid_cr']]

#Splitting of dataset
train=Z1.iloc[0:961]
train=np.array(train)
test=np.array(Z2.iloc[0:961])
eval=np.array(Z1.iloc[962:])


#Construction of neural network model
model=tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(5000, 50, input_length=14))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=1024,input_shape=[14]))
model.add(tf.keras.layers.Dense(units=546,activation='relu'))
model.add(layers.Dropout(0.25))
model.add(tf.keras.layers.Dense(units=200,activation='relu'))
model.add(layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(units=300,activation='relu'))
model.add(layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(units=120,activation='relu'))
model.add(layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(units=8,activation='relu'))
model.add(tf.keras.layers.Dense(units=3,activation='softmax'))

#Compilation of the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

#Training and evaluating the model
model.fit(train,test,epochs=16,validation_split=0.2,verbose=1)
model.evaluate(train,test)

#Predicting output for test dataset
result=model.predict_classes(eval)
#result
lookup = {1:"TRUE",0:"FALSE"}
result = [lookup[i] for i in result]


#creating two different dataframes to bring up the sample submission
df2 = pd.DataFrame(result)
df1= pd.DataFrame(Z['record_id'].iloc[965:])

#Concatenating the dataframes
data = [df1, df2]
headers = ["record_id", "Conversion_fraud"]
df3 = pd.concat(data,axis=1, keys=headers)

#Predicted dataframe
df3

#Saving the df3 as csv file
df3.to_csv ('content\drive\predict.csv', index=None)